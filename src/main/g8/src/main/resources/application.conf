
bus{
  kafka = "localhost:9092"
  kinesis{
    stream = "typebus"
    endpoint = "https://localhost:4567"
    region = "us-west-2"
    shards = ["shardId-000000000000"]
    dynamo.endpoint="http://localhost:8000"
    disable-cloudwatch-metrics = true
  }
}
# adjusting logging for entire Kafka
log4j.logger.org.apache.kafka=WARN

akka {
  loglevel = INFO
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  nr-event-consumer-instances = 10

  log-dead-letters-during-shutdown = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }


  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    roles = ["$name;format="normalize"$"]
    seed.zookeeper {
      url = "127.0.0.1:2181"
      path = "/akka/cluster/seed"
    }

    auto-down-unreachable-after = 2s
    sharding {

      # The extension creates a top level actor with this name in top level system scope,
      # e.g. '/system/sharding'
      guardian-name = sharding

      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = "$name;format="normalize"$"

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration, with an exponential back-off
      # of up to 5 times this duration.
      coordinator-failure-backoff = 5 s

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s

      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000

      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # Time given to a region to acknowledge it's hosting a shard.
      shard-start-timeout = 10 s

      # If the shard is remembering entities and can't store state changes
      # will be stopped and then started again after this duration. Any messages
      # sent to an affected entity may be lost in this process.
      shard-failure-backoff = 10 s

      # If the shard is remembering entities and an entity stops itself without
      # using passivate. The entity will be restarted after this duration or when
      # the next message for it is received, which ever occurs first.
      entity-restart-backoff = 10 s

      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      #journal-plugin-id = "akka.persistence.journal.dummy-journal"
      journal-plugin-id = "my-dynamodb-journal"

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      snapshot-plugin-id = ""

      # Parameter which determines how the coordinator will be store a state
      # valid values either "persistence" or "ddata"
      # The "ddata" mode is experimental, since it depends on the experimental
      # module akka-distributed-data-experimental.
      state-store-mode = "persistence"

      # The shard saves persistent snapshots after this number of persistent
      # events. Snapshots are used to reduce recovery times.
      snapshot-after = 1000

      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10

        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }

      # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
      # works only for state-store-mode = "ddata"
      waiting-for-state-timeout = 5 s

      # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
      # works only for state-store-mode = "ddata"
      updating-state-timeout = 5 s

      # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
      coordinator-singleton = \${akka.cluster.singleton}

      # The id of the dispatcher to use for ClusterSharding actors.
      # If not specified default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      # This dispatcher for the entity actors is defined by the user provided
      # Props, i.e. this dispatcher is not used for the entity actors.
      use-dispatcher = ""
    }

  }

  $if(route_support.truthy)$
  http{
    server {
      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = infinite

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = true
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }
    }
  }
  $endif$
}

akka.persistence.journal.plugin = "my-dynamodb-journal"

my-dynamodb-journal = \${dynamodb-journal} # include the default settings
my-dynamodb-journal {                     # and add some overrides
  journal-table =  "prs-journal" #<the name of the table to be used>
  journal-name =  "jrn_" # <prefix to be used for all keys stored by this plugin>
  aws-access-key-id =  "0"
  aws-secret-access-key =  "0"
  endpoint =  "http://localhost:8000" #"https://dynamodb.us-east-1.amazonaws.com" # or where your deployment is
}

$if(route_support.truthy)$
default-listener {

  # All squbs listeners carry the type "squbs.listener"
  type = squbs.listener

  # Add aliases for the listener in case the cube's route declaration binds to a listener with a different name.
  # Just comma separated names are good, like...
  # aliases = [ foo-listener, bar-listener ]
  aliases = [admin-listener]

  # Service bind to particular address/interface. The default is 0.0.0.0 which is any address/interface.
  bind-address = "0.0.0.0"

  # Whether or not using full host name for address binding
  full-address = false

  # Service bind to particular port. 8080 is the default.
  bind-port = 8181

  # Listener uses HTTPS?
  secure = false

  # HTTPS needs client authorization? This configuration is not read if secure is false.
  need-client-auth = false

  # Any custom SSLContext provider? Setting to "default" means platform default.
  ssl-context = default
}
$endif$

