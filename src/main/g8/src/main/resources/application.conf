
bus{
  kafka = "localhost:9092"
  kinesis{
    stream = "typebus"
    endpoint = "https://localhost:4567"
    region = "us-west-2"
    shards = ["shardId-000000000000"]
    dynamo.endpoint="http://localhost:8000"
    disable-cloudwatch-metrics = true
  }
}
# adjusting logging for entire Kafka
log4j.logger.org.apache.kafka=WARN

akka {
  loglevel = INFO
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  nr-event-consumer-instances = 10

  log-dead-letters-during-shutdown = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }


  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    roles = ["$name;format="normalize"$"]
    seed.zookeeper {
      url = "127.0.0.1:2181"
      path = "/akka/cluster/seed"
    }

    auto-down-unreachable-after = 2s
    sharding {

      # The extension creates a top level actor with this name in top level system scope,
      # e.g. '/system/sharding'
      guardian-name = sharding

      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = "$name;format="normalize"$"

      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # If the coordinator can't store state changes it will be stopped
      # and started again after this duration, with an exponential back-off
      # of up to 5 times this duration.
      coordinator-failure-backoff = 5 s

      # The ShardRegion retries registration and shard location requests to the
      # ShardCoordinator with this interval if it does not reply.
      retry-interval = 2 s

      # Maximum number of messages that are buffered by a ShardRegion actor.
      buffer-size = 100000

      # Timeout of the shard rebalancing process.
      handoff-timeout = 60 s

      # Time given to a region to acknowledge it's hosting a shard.
      shard-start-timeout = 10 s

      # If the shard is remembering entities and can't store state changes
      # will be stopped and then started again after this duration. Any messages
      # sent to an affected entity may be lost in this process.
      shard-failure-backoff = 10 s

      # If the shard is remembering entities and an entity stops itself without
      # using passivate. The entity will be restarted after this duration or when
      # the next message for it is received, which ever occurs first.
      entity-restart-backoff = 10 s

      # Rebalance check is performed periodically with this interval.
      rebalance-interval = 10 s

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      #journal-plugin-id = "akka.persistence.journal.dummy-journal"
      journal-plugin-id = "cassandra-journal"

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      snapshot-plugin-id = "cassandra-snapshot-store"

      # Parameter which determines how the coordinator will be store a state
      # valid values either "persistence" or "ddata"
      # The "ddata" mode is experimental, since it depends on the experimental
      # module akka-distributed-data-experimental.
      state-store-mode = "persistence"

      # The shard saves persistent snapshots after this number of persistent
      # events. Snapshots are used to reduce recovery times.
      snapshot-after = 1000

      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        rebalance-threshold = 10

        # The number of ongoing rebalancing processes is limited to this number.
        max-simultaneous-rebalance = 3
      }

      # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
      # works only for state-store-mode = "ddata"
      waiting-for-state-timeout = 5 s

      # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
      # works only for state-store-mode = "ddata"
      updating-state-timeout = 5 s

      # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
      coordinator-singleton = \${akka.cluster.singleton}

      # The id of the dispatcher to use for ClusterSharding actors.
      # If not specified default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      # This dispatcher for the entity actors is defined by the user provided
      # Props, i.e. this dispatcher is not used for the entity actors.
      use-dispatcher = ""
    }

  }

  $if(route_support.truthy)$
  http{
    server {
      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = infinite

      # Socket options to set for the listening socket. If a setting is left
      # undefined, it will use whatever the default on the system is.
      socket-options {
        so-receive-buffer-size = undefined
        so-send-buffer-size = undefined
        so-reuse-address = undefined
        so-traffic-class = undefined
        tcp-keep-alive = true
        tcp-oob-inline = undefined
        tcp-no-delay = undefined
      }
    }
  }
  $endif$
}

akka.persistence.journal.plugin = "cassandra-journal"
akka.persistence.snapshot-store.plugin = "cassandra-snapshot-store"


$if(route_support.truthy)$
default-listener {

  # All squbs listeners carry the type "squbs.listener"
  type = squbs.listener

  # Add aliases for the listener in case the cube's route declaration binds to a listener with a different name.
  # Just comma separated names are good, like...
  # aliases = [ foo-listener, bar-listener ]
  aliases = [admin-listener]

  # Service bind to particular address/interface. The default is 0.0.0.0 which is any address/interface.
  bind-address = "0.0.0.0"

  # Whether or not using full host name for address binding
  full-address = false

  # Service bind to particular port. 8080 is the default.
  bind-port = 8181

  # Listener uses HTTPS?
  secure = false

  # HTTPS needs client authorization? This configuration is not read if secure is false.
  need-client-auth = false

  # Any custom SSLContext provider? Setting to "default" means platform default.
  ssl-context = default
}
$endif$


cassandra-journal {

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # List of contact points in the Cassandra cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  # The value can be either a proper list, e.g. ["127.0.0.1", "127.0.0.2"],
  # or a comma-separated list within a single string, e.g. "127.0.0.1,127.0.0.2".
  contact-points = ["127.0.0.1"]

  # Port of contact points in the Cassandra cluster.
  # Will be ignored if the contact point list is defined by host:port pairs.
  port = 9042

  # The implementation of akka.persistence.cassandra.SessionProvider
  # is used for creating the Cassandra Session. By default the
  # the ConfigSessionProvider is building the Cluster from configuration properties
  # but it is possible to replace the implementation of the SessionProvider
  # to reuse another session or override the Cluster builder with other
  # settings.
  # For example, it is possible to lookup the contact points of the Cassandra cluster
  # asynchronously instead of giving them in the configuration in a subclass of
  # ConfigSessionProvider and overriding the lookupContactPoints method.
  # It may optionally have a constructor with an ActorSystem and Config parameter.
  # The config parameter is this config section of the plugin.
  session-provider = akka.persistence.cassandra.ConfigSessionProvider

  # The identifier that will be passed as parameter to the
  # ConfigSessionProvider.lookupContactPoints method.
  cluster-id = ""

  # Name of the keyspace to be created/used by the journal
  keyspace = "akka"

  # Parameter indicating whether the journal keyspace should be auto created.
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  keyspace-autocreate = true

  # Parameter indicating whether the journal tables should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  tables-autocreate = true

  # The number of retries when a write request returns a TimeoutException or an UnavailableException.
  write-retries = 3

  # Deletes are achieved using a metadata entry and then the actual messages are deleted asynchronously
  # Number of retries before giving up
  delete-retries = 3

  # The number of retries when a read query fails.
  read-retries = 3

  # Set this to a positive integer to enable speculative executions.
  # The value defines the number of speculative executions that will be
  # performed with the delay defined by 'speculative-executions-delay'.
  # See http://docs.datastax.com/en/developer/java-driver/3.1/manual/speculative_execution/
  # If you enable this you should also enable cassandra-query-journal.speculative-executions
  # which is used for the relay query.
  speculative-executions = 0

  # See 'speculative-executions'
  speculative-executions-delay = 1s

  # Number of retries before giving up connecting for the initial connection to the Cassandra cluster
  connect-retries = 3

  # Delay between connection retries, for the initial connection to the Cassandra cluster
  connect-retry-delay = 1s

  # Max delay of the ExponentialReconnectionPolicy that is used when reconnecting
  # to the Cassandra cluster
  reconnect-max-delay = 30s

  # Enable debug logging of queries as described in
  # https://docs.datastax.com/en/developer/java-driver/3.1/manual/logging/#logging-query-latencies
  log-queries = off

  # Akka-persistence allows multiple pending deletes for the same persistence id however this plugin only executes one
  # at a time per persistence id (deletes for different persistenceIds can happen concurrently).
  #
  # If multiple deletes for the same persistence id are received then they are queued.
  #
  # If the queue is full any subsequent deletes are failed immediately without attempting them in Cassandra.
  #
  # Deleting should be used with snapshots for efficiency for recovery thus deleting should be infrequent
  max-concurrent-deletes = 10

  # Cassandra driver connection pool settings
  # Documented at http://docs.datastax.com/en/developer/java-driver/latest/manual/pooling/
  connection-pool {

    # Create new connection threshold local
    new-connection-threshold-local = 800

    # Create new connection threshold remote
    new-connection-threshold-remote = 200

    # Connections per host core local
    connections-per-host-core-local = 1

    # Connections per host max local
    connections-per-host-max-local = 4

    # Connections per host core remote
    connections-per-host-core-remote = 1

    # Connections per host max remote
    connections-per-host-max-remote = 4

    # Max requests per connection local
    max-requests-per-connection-local = 32768

    # Max requests per connection remote
    max-requests-per-connection-remote = 2000

    # Sets the timeout when trying to acquire a connection from a host's pool
    pool-timeout-millis = 0

    # Sets the maximum number of requests that get enqueued if no connection is available.
    max-queue-size = 256
  }

  # Name of the table to be created/used by the journal.
  # If the table doesn't exist it is automatically created.
  table = "messages"

  # Compaction strategy for the journal table.
  # Please refer to the tests for example configurations.
  # Refer to http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compactSubprop.html
  # for more information regarding the properties.
  table-compaction-strategy {
    class = "SizeTieredCompactionStrategy"
  }

  # Name of the table to be created/used for storing metadata.
  # If the table doesn't exist it is automatically created.
  metadata-table = "metadata"

  # Name of the table to be created/used for journal config.
  # If the table doesn't exist it is automatically created.
  config-table = "config"

  # TODO remove this and just query cassandra to see what version it is
  # Set this to on to only use Cassandra 2.x compatible features,
  # i.e. if you are using a Cassandra 2.x server.
  # To run tests with Cassandra 2.x server you have to do the following:
  # - start Cassandra 2.x server on default port 9042, with empty data directory
  # - change CassandraLauncher.randomPort to 9042
  # - change CassandraLauncher.start to do nothing
  # - set this cassandra-2x-compat = on
  # - note that you must delete all data between each test run
  cassandra-2x-compat = off

  events-by-tag {
    # Enable/disable events by tag. If eventsByTag queries aren't required then this should be set to
    # false to avoid the overhead of maintaining the tag_views table.
    enabled = true

    # Tagged events are written to a separate Cassandra table in unlogged batches
    # Max size of these batches. The best value for this will depend on the size of
    # the serialized events. Cassandra logs a warning for batches above a certain
    # size and this should be reduced if that warning is seen.
    max-message-batch-size = 150

    # Max time to buffer events for before writing.
    # Larger valeues will increase cassandra write efficiency but increase the delay before
    # seeing events in EventsByTag queries.
    # Setting this to 0 means that tag writes will get written immediately but will still be asynchronous
    # with respect to the PersistentActor's persist call. However, this will be very bad for throughput.
    flush-interval = 250ms

    # Update the tag_scanning table with this interval. Shouldn't be done too often to
    # avoid unecessary load. The tag_scanning table keeps track of a starting point for tag
    # scanning during recovery of persistent actor.
    scanning-flush-interval = 30s

    table = "tag_views"
    gc-grace-seconds = 864000
    compaction-strategy {
      class = "SizeTieredCompactionStrategy"
      # If setting a time-to-live then consider using TimeWindowCompactionStratery
      # See [here](http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html) for guideance.
      # It is reccommended not to have more than 50 buckets so this needs to be based on your
      # time-to-live e.g. if you set the TTL to 50 hours and the compaction window to 1 hour
      # there will be 50 buckets.
      # class = "TimeWindowCompactionStrategy"
      # compaction_window_unit = "HOURS"
      # compaction_window_size = 1
    }

    # How long events are kept for in the tag_views table
    # By default the events are kept for ever. Uncomment and set to an appropriate
    # duration for your use case. See the compaction-strategy.class if you set this
    #time-to-live = 2d

    # WARNING: Can not be changed after data has been written
    #
    # Unless you have a significant (million+) of events for a single tag
    # do not change this to Minute. Each tag in the tag-views table has a partition
    # per tag per bucket
    # Valid options: Day, Hour, Minute
    bucket-size = "Hour"
  }

  # meta columns were added in version 0.55. If you don't alter existing messages table and still
  # use `tables-autocreate=on` you have to set this property to off.
  # When trying to create the materialized view with the meta columns before corresponding columns
  # have been added the messages table an exception "Undefined column name meta_ser_id" is raised,
  # because Cassandra validates the "CREATE MATERIALIZED VIEW IF NOT EXISTS"
  # even though the view already exists and will not be created. To work around that issue you can disable the
  # meta data columns in the materialized view by setting this property to off.
  meta-in-events-by-tag-view = on

  # replication strategy to use. SimpleStrategy or NetworkTopologyStrategy
  replication-strategy = "SimpleStrategy"

  # Replication factor to use when creating a keyspace. Is only used when replication-strategy is SimpleStrategy.
  replication-factor = 1

  # Replication factor list for data centers. Is only used when replication-strategy is NetworkTopologyStrategy.
  # The value can be either a proper list, e.g. ["dc1:3", "dc2:2"],
  # or a comma-separated list within a single string, e.g. "dc1:3,dc2:2".
  data-center-replication-factors = []

  # To limit the Cassandra hosts this plugin connects with to a specific datacenter.
  # (DCAwareRoundRobinPolicy withLocalDc)
  # The id for the local datacenter of the Cassandra hosts it should connect to.
  # By default, this property is not set resulting in Datastax's standard round robin policy being used.
  local-datacenter = ""

  # Number of hosts from non-local datacenter to use as a fall-back policy.
  # Works only when local-datacenter is set
  used-hosts-per-remote-dc = 0

  # To connect to the Cassandra hosts with credentials.
  # Authentication is disabled if username is not configured.
  authentication.username = ""
  authentication.password = ""

  # SSL can be configured with the following properties.
  # SSL is disabled if the truststore is not configured.
  # For detailed instructions, please refer to the DataStax Cassandra chapter about
  # SSL Encryption: http://docs.datastax.com/en/cassandra/2.0/cassandra/security/secureSslEncryptionTOC.html
  # Path to the JKS Truststore file
  ssl.truststore.path = ""
  # Password to unlock the JKS Truststore
  ssl.truststore.password = ""
  # Path to the JKS Keystore file (optional config, only needed for client authentication)
  ssl.keystore.path = ""
  # Password to unlock JKS Truststore and access the private key (both must use the same password)
  ssl.keystore.password = ""

  # Write consistency level
  # The default read and write consistency levels ensure that persistent actors can read their own writes.
  # During normal operation, persistent actors only write to the journal, reads occur only during recovery.
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of messages that will be batched when using `persistAsync`.
  # Also used as the max batch size for deletes.
  max-message-batch-size = 100

  # Target number of entries per partition (= columns per row).
  # Must not be changed after table creation (currently not checked).
  # This is "target" as AtomicWrites that span partition boundaries will result in bigger partitions to ensure atomicity.
  target-partition-size = 500000

  # Maximum size of result set
  max-result-size = 250

  # Maximum size of result set during replay
  max-result-size-replay = 250

  # The query journal to use when recovering
  query-plugin = "cassandra-query-journal"

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"

  # The time to wait before cassandra will remove the tombstones created for deleted entries.
  # cfr. gc_grace_seconds table property documentation on
  # http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/tabProp.html
  gc-grace-seconds = 864000

  # Enable DistributedPubSub to announce events for a specific tag have
  # been written. These announcements cause any ongoing getEventsByTag to immediately re-poll, rather than
  # wait. In order enable this feature, make the following settings:
  #
  #    - enable clustering for your actor system
  #    - cassandra-journal.pubsub-notification=on              (send real-time announcements at most every sec)
  #    - cassandra-query-journal.events-by-tag.eventual-consistency-delay = 0s
  #
  # Setting pubsub-notification to "off" will disable the journal sending these announcements.
  pubsub-notification = off

  # Set the protocol version explicitly, should only be used for compatibility testing.
  # Supported values: 3, 4
  protocol-version = ""

  # Options to configure low-level socket options for the connections to Cassandra hosts
  # See: https://datastax.github.io/java-driver/manual/socket_options
  socket {

    # how long the driver waits to establish a new connection to a Cassandra node before giving up
    connection-timeout-millis = 5000

    # the per-host read timeout in milliseconds. Should be higher than the timeout settings used on the Cassandra side
    read-timeout-millis = 12000

    # a hint to the size of the underlying buffers for outgoing network I/O. Set to zero to
    # use the default from the underlying Netty transport (Java NIO or native epoll)
    send-buffer-size = 0

    # a hint to the size of the underlying buffers for incoming network I/O. Set to zero to
    # use the default from the underlying Netty transport (Java NIO or native epoll)
    receive-buffer-size = 0
  }


}

# This configures the default settings for all Cassandra Snapshot plugin
# instances in the system. If you are using just one configuration for
# all persistent actors then you should point your akka.persistence.snapshot-store.plugin
# setting to this section.
#
# Otherwise you need to create differently named sections containing
# only those settings that shall be different from the defaults
# configured here, importing the defaults like so:
#
#   my-cassandra-snapshot-store = \${cassandra-snapshot-store}
#   my-cassandra-snapshot-store {
#     <settings...>
#   }
cassandra-snapshot-store {

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Comma-separated list of contact points in the Cassandra cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  contact-points = ["127.0.0.1"]

  # Port of contact points in the Cassandra cluster.
  # Will be ignored if the contact point list is defined by host:port pairs.
  port = 9042

  # The implementation of akka.persistence.cassandra.SessionProvider
  # is used for creating the Cassandra Session. By default the
  # the ConfigSessionProvider is building the Cluster from configuration properties
  # but it is possible to replace the implementation of the SessionProvider
  # to reuse another session or override the Cluster builder with other
  # settings.
  # For example, it is possible to lookup the contact points of the Cassandra cluster
  # asynchronously instead of giving them in the configuration in a subclass of
  # ConfigSessionProvider and overriding the lookupContactPoints method.
  # It may optionally have a constructor with an ActorSystem and Config parameter.
  # The config parameter is this config section of the plugin.
  session-provider = akka.persistence.cassandra.ConfigSessionProvider

  # The identifier that will be passed as parameter to the
  # ConfigSessionProvider.lookupContactPoints method.
  cluster-id = ""

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "akka_snapshot"

  # Parameter indicating whether the snapshot keyspace should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  keyspace-autocreate = true

  # Parameter indicating whether the snapshot tables should be auto created
  # Not all Cassandra settings are configurable when using autocreate and for
  # full control of the keyspace and table definitions you should create them
  # manually (with a script).
  tables-autocreate = true

  # The number of retries when a write request returns a TimeoutException or an UnavailableException.
  write-retries = 3

  # Number of retries before giving up
  delete-retries = 3

  # The number of retries when a read query fails.
  read-retries = 3

  # Set this to a positive integer to enable speculative executions.
  # The value defines the number of speculative executions that will be
  # performed with the delay defined by 'speculative-executions-delay'.
  # See http://docs.datastax.com/en/developer/java-driver/3.1/manual/speculative_execution/
  speculative-executions = 0

  # See 'speculative-executions'
  speculative-executions-delay = 1s

  # Number of retries before giving up connecting for the initial connection to the Cassandra cluster
  connect-retries = 3

  # Delay between connection retries, for the initial connection to the Cassandra cluster
  connect-retry-delay = 1s

  # Max delay of the ExponentialReconnectionPolicy that is used when reconnecting
  # to the Cassandra cluster
  reconnect-max-delay = 30s

  # Enable debug logging of queries as described in
  # https://docs.datastax.com/en/developer/java-driver/3.1/manual/logging/#logging-query-latencies
  log-queries = off

  # Cassandra driver connection pool settings
  # Documented at http://docs.datastax.com/en/developer/java-driver/latest/manual/pooling/
  # and http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/PoolingOptions.html
  connection-pool {

    # Create new connection threshold local
    new-connection-threshold-local = 50

    # Create new connection threshold remote
    new-connection-threshold-remote = 50

    # Connections per host core local
    connections-per-host-core-local = 1

    # Connections per host max local
    connections-per-host-max-local = 4

    # Connections per host core remote
    connections-per-host-core-remote = 1

    # Connections per host max remote
    connections-per-host-max-remote = 4

    # Max requests per connection local
    max-requests-per-connection-local = 32768

    # Max requests per connection remote
    max-requests-per-connection-remote = 2000

    # Sets the timeout when trying to acquire a connection from a host's pool
    pool-timeout-millis = 0

    # Sets the maximum number of requests that get enqueued if no connection is available.
    max-queue-size = 256
  }

  # Name of the table to be created/used by the snapshot store.
  # If the table doesn't exist it is automatically created.
  table = "snapshots"

  # Compaction strategy for the snapshot table
  # Please refer to the tests for example configurations.
  # Refer to http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compactSubprop.html
  # for more information regarding the properties.
  table-compaction-strategy {
    class = "SizeTieredCompactionStrategy"
  }

  # Name of the table to be created/used for journal config.
  # If the table doesn't exist it is automatically created.
  config-table = "config"

  # Name of the table to be created/used for storing metadata.
  # If the table doesn't exist it is automatically created.
  metadata-table = "metadata"

  # replication strategy to use. SimpleStrategy or NetworkTopologyStrategy
  replication-strategy = "SimpleStrategy"

  # Replication factor to use when creating a keyspace. Is only used when replication-strategy is SimpleStrategy.
  replication-factor = 1

  # Replication factor list for data centers, e.g. ["dc1:3", "dc2:2"]. Is only used when replication-strategy is NetworkTopologyStrategy.
  data-center-replication-factors = []

  # To limit the Cassandra hosts this plugin connects with to a specific datacenter.
  # (DCAwareRoundRobinPolicy withLocalDc)
  # The id for the local datacenter of the Cassandra hosts it should connect to.
  # By default, this property is not set resulting in Datastax's standard round robin policy being used.
  local-datacenter = ""

  # Number of hosts from non-local datacenter to use as a fall-back policy.
  # Works only when local-datacenter is set
  used-hosts-per-remote-dc = 0

  # To connect to the Cassandra hosts with credentials.
  # Authentication is disabled if username is not configured.
  authentication.username = ""
  authentication.password = ""

  # SSL can be configured with the following properties.
  # SSL is disabled if the truststore is not configured.
  # For detailed instructions, please refer to the DataStax Cassandra chapter about
  # SSL Encryption: http://docs.datastax.com/en/cassandra/2.0/cassandra/security/secureSslEncryptionTOC.html
  # Path to the JKS Truststore file
  ssl.truststore.path = ""
  # Password to unlock the JKS Truststore
  ssl.truststore.password = ""
  # Path to the JKS Keystore file (optional config, only needed for client authentication)
  ssl.keystore.path = ""
  # Password to unlock JKS Truststore and access the private key (both must use the same password)
  ssl.keystore.password = ""

  # Write consistency level
  write-consistency = "ONE"

  # Read consistency level
  read-consistency = "ONE"

  # Maximum size of result set
  max-result-size = 50001

  # Dispatcher for the plugin actor and task.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"

  # Set the protocol version explicitly, should only be used for compatibility testing.
  # Supported values: 3, 4
  protocol-version = ""

  # The time to wait before cassandra will remove the tombstones created for deleted entries.
  # cfr. gc_grace_seconds table property documentation on
  # http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/tabProp.html
  gc-grace-seconds = 864000

  # Options to configure low-level socket options for the connections to Cassandra hosts
  # See: http://docs.datastax.com/en/developer/java-driver/latest/manual/socket_options
  socket {

    # how long the driver waits to establish a new connection to a Cassandra node before giving up
    connection-timeout-millis = 5000

    # the per-host read timeout in milliseconds. Should be higher than the timeout settings used on the Cassandra side
    read-timeout-millis = 12000

    # a hint to the size of the underlying buffers for outgoing network I/O. Set to zero to
    # use the default from the underlying Netty transport (Java NIO or native epoll)
    send-buffer-size = 0

    # a hint to the size of the underlying buffers for incoming network I/O. Set to zero to
    # use the default from the underlying Netty transport (Java NIO or native epoll)
    receive-buffer-size = 0
  }

  # Number load attempts when recovering from the latest snapshot fails
  # yet older snapshot files are available. Each recovery attempt will try
  # to recover using an older than previously failed-on snapshot file
  # (if any are present). If all attempts fail the recovery will fail and
  # the persistent actor will be stopped.
  max-load-attempts = 3
}

# This configures the default settings for all CassandraReadJournal plugin
# instances in the system.
#
# If you use multiple plugin instances you need to create differently named
# sections containing only those settings that shall be different from the defaults
# configured here, importing the defaults like so:
#
#   my-cassandra-query-journal = \${cassandra-query-journal}
#   my-cassandra-query-journal {
#     <settings...>
#   }
cassandra-query-journal {
  # Implementation class of the Cassandra ReadJournalProvider
  class = "akka.persistence.cassandra.query.CassandraReadJournalProvider"

  # Absolute path to the write journal plugin configuration section
  write-plugin = "cassandra-journal"

  # New events are retrieved (polled) with this interval.
  refresh-interval = 3s

  # Sequence numbers for a persistenceId is assumed to be monotonically increasing
  # without gaps. That is used for detecting missing events.
  # In early versions of the journal that might not be true and therefore
  # this can be relaxed by setting this property to off.
  gap-free-sequence-numbers = on

  # When using LQ writing in one DC and querying in another, the events for an entity may
  # appear in the querying DC out of order, when that happens, try for this amount of
  # time to find the in-order sequence number before failing the stream
  events-by-persistence-id-gap-timeout = 10s

  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = 500

  # The fetch size of the Cassandra select statement
  # Value less or equal to 0 means max-result-size will be used
  # http://docs.datastax.com/en/drivers/java/3.0/com/datastax/driver/core/Statement.html
  max-result-size-query = 250

  # When the available rows without fetching in the ResultSet reach this threshold
  # more will be fetched asynchronously.
  # Value should be between 0.0 and 1.0.
  # E.g. threshold of 0.1 and fetch size of 250 will trigger new fetch when
  # 25 rows are available.
  fetch-more-threshold = 0.2

  # Read consistency level
  read-consistency = "QUORUM"

  # The number of retries when a read query fails.
  read-retries = 3

  # Set this to a positive integer to enable speculative executions.
  # The value defines the number of speculative executions that will be
  # performed with the delay defined by 'speculative-executions-delay'.
  # See http://docs.datastax.com/en/developer/java-driver/3.1/manual/speculative_execution/
  speculative-executions = 0

  # See 'speculative-executions'
  speculative-executions-delay = 1s

  # Configure this to the first bucket eventByTag queries will start from in the format
  # yyyyMMddTHH:mm yyyyMMdd is also supported if using Day as a bucket size
  # Will be rounded down to the start of whatever time bucket it falls into
  # When NoOffset is used it will look for events from this day and forward.
  first-time-bucket = "20151120T00:00"

  events-by-tag {
    # How long to look for delayed events
    # This works by adding an additional (internal) sequence number to each tag / persistence id
    # event stream so that the read side can detect missing events. When a gap is detected no new events
    # are emitted from the stream until either the missing events are found or the timeout is reached
    # If the event is not found it is checked every `refresh-interval` so do not set this lower than that
    # if you want at least one retry
    # When looking for missing events only the current time bucket and the previous bucket are checked meaning that if
    # clocks are out of sync, or cassandra replication is out by more than your bucket size (minute, hour or day)
    # then the missing events won't be found
    gap-timeout = 10s

    # When a new persistenceId is found in an eventsByTag query that wasn't found in the initial offset scanning
    # period as it didn't have any events in the current time bucket, this is how long the stage will delay events
    # looking for any smaller tag pid sequence nrs. 0s means that the found event is assumed to be the first.
    # The edge case is if events for a not previously seen persistenceId come out of order then if this is set to
    # 0s the newer event will be delivered and when the older event is found the stream will fail as events have
    # to be delivered in order.
    new-persistence-id-scan-timeout = 100ms

    # For offset queries that start in the current time bucket a period of scanning
    # takes place before deliverying events to look for the lowest sequence number
    # for each persistenceId. Any value above 0 will result in at least one scan from
    # the offset to (offset + period). Larger values will result in a longer period of time
    # before the stream starts emitting events.
    offset-scanning-period = 200ms

    # For eventsByTag queries how long to delay the query for. For event writes that come from different nodes
    # the clocks may be out of sync meaning events aren't received in order. If the events are delivered to the
    # query immediately the offset may be greater than some delayed events. Meaning that if this offset is saved
    # for restarting the query the delayed events will never be processed.
    eventual-consistency-delay = 5s

    # Verbose debug logging for offset updates for events by tag queries. Any logging that is per event is
    # affected by this flag. Debug logging that is per query to Cassandra is enabled if DEBUG logging is enabled
    verbose-debug-logging = false
  }

  # Deserialization of events is perfomed in an Akka streams mapAsync operator and this is the
  # parallelism for that. Increasing to means that deserialization is pipelined, which can
  # be an advantage for machines with many CPU cores but otherwise it might be slower because
  # of higher CPU saturation and more competing tasks when there are many concurrent queries or
  # replays.
  deserialization-parallelism = 1

  # Dispatcher for the plugin actors.
  plugin-dispatcher = "cassandra-plugin-default-dispatcher"
}

# Default dispatcher for plugin actor and tasks.
cassandra-plugin-default-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 6
    parallelism-factor = 1
    parallelism-max = 6
  }
}



