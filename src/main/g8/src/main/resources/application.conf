bus{
  kafka{
    bootstrap-servers = "localhost:9092"
    bootstrap-servers = \${?KAFKA_BOOTSTRAP}
  }
  trace = false
  trace = \${?TYPEBUS_TRACE}
}

akka {
  cluster.enable-cluster-bootstrap = true

  // cluster bootstrap should use kubernetes api
  management {
    cluster.bootstrap.contact-point-discovery {
      discovery-method = kubernetes-api
      # pod port name for Akka Management HTTP
      port-name = "management"

      #Wait until there are NR contact points present before attempting initial cluster formation
      required-contact-point-nr = 1
      required-contact-point-nr = \${?REQUIRED_CONTACTS}
    }
    http {
      port = 8558
      bind-hostname = "0.0.0.0"
    }
  }

  discovery {
    // service discovery should use akka-dns
    method = akka-dns
    kubernetes-api {
      pod-namespace = "default"
      pod-port-name = "management"
    }
  }
}

# adjusting logging for entire Kafka
log4j.logger.org.apache.kafka=WARN

akka {
  loglevel = INFO
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  nr-event-consumer-instances = 10

  log-dead-letters-during-shutdown = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }

  artery {
    enabled = on
    transport = tcp
    #      canonical.hostname = "127.0.0.1"
    canonical.port = 2551
  }

  persistence{
    journal.plugin = "cassandra-journal"
    snapshot-store.plugin = "cassandra-snapshot-store"
  }

  cluster {
    roles = ["$name;format="normalize"$"]
    seed-nodes = []
    seed-nodes = \${?SEED_NODES}  # this MUST be [] when using k8s discovery
    shutdown-after-unsuccessful-join-seed-nodes = 120s

    sharding{
      # Prefer 'ddata' over 'persistence' to share cluster sharding state for new projects.
      # See https://doc.akka.io/docs/akka/current/cluster-sharding.html#distributed-data-vs-persistence-mode
      state-store-mode = ddata

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      #journal-plugin-id = "akka.persistence.journal.dummy-journal"
      journal-plugin-id = cassandra-journal

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      snapshot-plugin-id = cassandra-snapshot-store
    }
  }
}

akka.coordinated-shutdown.exit-jvm = on

######################################
# Persistence (Cassandra) Configuration
######################################

//Use same keyspace for journal, snapshot-store and read-side.
//Each service should be in a unique keyspace.
$name;format="normalize"$.cassandra.keyspace = $name;format="normalize"$

cassandra-journal{
  keyspace = \${$name;format="normalize"$.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}
cassandra-snapshot-store{
  keyspace = \${$name;format="normalize"$.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
  contact-points = ["127.0.0.1"]
  contact-points = \${?CASSANDRA_CONTACT_POINTS}
}

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
  "akka.Done"                 = akka-misc
  "akka.actor.Address"        = akka-misc
  "akka.remote.UniqueAddress" = akka-misc
}


# https://discuss.lightbend.com/t/no-configuration-setting-found-for-key-decode-max-size/2738/3
akka.http.routing.decode-max-size = 8m

cassandra-query-journal.refresh-interval = 1s

lagom.persistence {

  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 100

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  # Author note: snapshotting turned off
  snapshot-after = off

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  # Author note: Set to one day - this may be a bit long for production.
  passivate-after-idle-timeout = 86400s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  # Author note: Made longer to support potentially slower Minikube environment
  ask-timeout = 60s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}


